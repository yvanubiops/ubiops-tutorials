{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Llama 3 8B to UbiOps\n",
    "\n",
    "This notebook will show you how you can create a cloud-based inference API endpoint for the \n",
    "[Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) model using UbiOps. The Llama model \n",
    "is already pre-trained and will be loaded from the Huggingface [meta-llama](https://huggingface.co/meta-llama) library. \n",
    "Note that downloading this model requires you to have a Hugginface token that has sufficient permissionsto download Llama 3.\n",
    "\n",
    "The [Meta Llama 3 models](https://llama.meta.com/llama3/) are a collection of pre-trained and instruction tuned generative \n",
    "text models, in 8B and 70B parameter sizes. The instruction versions of these models are optimized for dialogue use cases. \n",
    "Meta claims that Llama 3B outperforms models with a similar size, like Mistral 7B & Gemma 7B on common industry benchmarks. \n",
    "The model deployed in this tutorial is the instruction tuned version of the Llama 8B model. We also optimize the inference \n",
    "speed using the [flash attention library](https://github.com/Dao-AILab/flash-attention).\n",
    "\n",
    "In this notebook, we will walk you through:\n",
    "\n",
    "1. Connecting with the UbiOps API client\n",
    "2. Creating a code environment for our deployment\n",
    "3. Creating a deployment for the Llama-3-8B-Instruct\n",
    "4. Calling the Llama 3 deployment endpoint\n",
    "\n",
    "Llama 3 is a text-to-text model. Therefore we will make a deployment that takes a text prompt as input, and returns a \n",
    "response. Next to the user's input, we will also add the `system_prompt` and `config` to the deployment's input. Using\n",
    "this set-up enables you to experiment with different system prompts and generation parameters to see how they affect the \n",
    "responses of the model.\n",
    "\n",
    "\n",
    "The deployment will return the `input`, which is the user's `prompt` and `system_prompt`, and the `used_config`.\n",
    "\n",
    "Default pre-set values will be used for the `system_prompt` and `config` if these are not provided, these can be found\n",
    "in the `__init__` statement of the `deployment.py`.\n",
    "\n",
    "|Deployment input & output variables| **Variable name** |**Data type** |\n",
    "|--------------------|--------------|--------------|\n",
    "| **Input fields**   | prompt | string |\n",
    "|                    | system_prompt | string |\n",
    "|                    | config | dictionary|\n",
    "| **Output fields**  | output | string |\n",
    "|                    | input        | string |\n",
    "|                    | used_config  | dictionary |\n",
    "\n",
    "Note that we deploy to a GPU instance by default, which is not accessible in every project. You can \n",
    "[contact us](https://ubiops.com/contact-us/) about this.\n",
    "\n",
    "Let's start coding!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up a connection with the UbiOps API client\n",
    "\n",
    "To use the UbiOps API from our notebook, we need to install the UbiOps Python Client Library first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU ubiops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up a connection with your UbiOps environment. To do this we will need the name of your UbiOps project and \n",
    "an API token with the `project_editor` permissions.\n",
    "\n",
    "You can paste your project name and API token in the code block below before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ubiops\n",
    "from datetime import datetime\n",
    "\n",
    "API_TOKEN = \"<INSERT API_TOKEN WITH PROJECT EDITOR RIGHTS>\"  # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"<INSERT PROJECT NAME IN YOUR ACCOUNT>\"\n",
    "\n",
    "HF_TOKEN = \"<INSERT HUGGINGFACE TOKEN WITH CORRECT ACCESS>\"  # We need this token to download the model from Huggingface \n",
    "\n",
    "DEPLOYMENT_NAME = f\"llama-3-8b-{datetime.now().date()}\"\n",
    "DEPLOYMENT_VERSION = \"v1\"\n",
    "\n",
    "# Initialize client library\n",
    "configuration = ubiops.Configuration(host=\"https://api.ubiops.com/v2.1\")\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "# Establish a connection\n",
    "client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.CoreApi(client)\n",
    "print(api.projects_get(PROJECT_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the environment\n",
    "\n",
    "[The environment](https://ubiops.com/docs/environments/#environments) that our model runs in can be managed separately. To do this we need to select a base environment, to which\n",
    "we will add additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_dir = \"environment_package\"\n",
    "ENVIRONMENT_NAME = \"llama-3-environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir {environment_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the Python packages required to run the model in a `requirements.txt`, which we will later upload to UbiOps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {environment_dir}/requirements.txt\n",
    "# This file contains package requirements for the environment\n",
    "# Installed via PIP.\n",
    "torch==2.0.1+cu118\n",
    "huggingface-hub==0.20.3\n",
    "transformers==4.38.1\n",
    "scipy\n",
    "diffusers\n",
    "safetensors\n",
    "ninja\n",
    "jupyterlab==4.0.11\n",
    "notebook==7.0.7\n",
    "ipywidgets\n",
    "https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.7/flash_attn-2.5.7+cu118torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a `ubiops.yaml` to set a remote pip index. This ensures that we will install a CUDA-compatible version\n",
    "of PyTorch. CUDA allows models to be loaded and to run on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {environment_dir}/ubiops.yaml\n",
    "environment_variables:\n",
    "- PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a custom environment on UbiOps. We select `Ubuntu 22.04 + Python 3.10 + CUDA 11.7.1` as the `base_environment`,\n",
    "and add the additional dependencies we defined earlier to the `base_environment` to create a `custom environment`. The\n",
    "environment will be called `llama-3-environment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = api.environments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=ubiops.EnvironmentCreate(\n",
    "        name=ENVIRONMENT_NAME,\n",
    "        # display_name=ENVIRONMENT_NAME,\n",
    "        base_environment=\"ubuntu22-04-python3-10-cuda11-7-1\", \n",
    "        description=\"Environment to run Llama-3 from Huggingface\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package and upload the environment files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "training_environment_archive = shutil.make_archive(\n",
    "    environment_dir, \"zip\", \".\", environment_dir\n",
    ")\n",
    "api.environment_revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    environment_name=ENVIRONMENT_NAME,\n",
    "    file=training_environment_archive,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a deployment for the Llama 3 8B Instruct model\n",
    "\n",
    "With the environment set up, we can start writing the code to run the Llama-3-8B model, and push it to UbiOps.\n",
    "\n",
    "We will create a `deployment.py` with a `Deployment` class, which has two methods:\n",
    "\n",
    "- The `__init__`which will run when the deployment starts up. This method can be used to load models, data artefacts and \n",
    "other requirements for inference.\n",
    "- The `request()` will run every time a call is made to the models REST API endpoint and includes all the logic for \n",
    "processing data.\n",
    "\n",
    "Separating the logic between the two methods will ensure fast model response times. The model will be loaded in the \n",
    "`__init__` method, and the code that needs to be run when a call is made to the deployment in the `request()` method.\n",
    "This way the model only needs to be loaded in when the deployment starts up.\n",
    "\n",
    "As mentioned in the introduction, we will add a default `system_prompt` and `config` to the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_code_dir = \"deployment_code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {deployment_code_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {deployment_code_dir}/deployment.py\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    LlamaForCausalLM, \n",
    "    GenerationConfig\n",
    ")\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. Any code inside this method will execute when the deployment starts up.\n",
    "        It can for example be used for loading modules that have to be stored in memory or setting up connections.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising deployment\")\n",
    "        \n",
    "        # Read out model-related environment variables\n",
    "        LLAMA_VERSION = os.environ.get('LLAMA_VERSION', 'meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "        self.REPETITION_PENALTY = float(os.environ.get('REPETITION_PENALTY', 1.15))\n",
    "        self.MAX_RESPONSE_LENGTH  = float(os.environ.get('MAX_RESPONSE_LENGTH', 256))\n",
    "        \n",
    "        # Login to Huggingface\n",
    "        HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "        login(token=HF_TOKEN)\n",
    "\n",
    "   \n",
    "        print(\"Downloading tokenizer\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(LLAMA_VERSION, \n",
    "                                                       device_map = 'auto'\n",
    "        )\n",
    "        self.model = LlamaForCausalLM.from_pretrained(LLAMA_VERSION, \n",
    "                                                      torch_dtype = torch.float16, \n",
    "                                                      device_map = 'auto', \n",
    "                                                      use_safetensors = True,\n",
    "                                                      attn_implementation=\"flash_attention_2\",\n",
    "        )\n",
    "\n",
    "  \n",
    "\n",
    "        self.pipe = pipeline(\n",
    "            os.environ.get(\"PIPELINE_TASK\", \"text-generation\"),\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            return_full_text=False,\n",
    "        )\n",
    "        \n",
    "        # Set default prompt generation variables\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"content\": \"{system_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": \"{user_prompt}\"},\n",
    "        ]\n",
    "        \n",
    "\n",
    "        self.terminators = [\n",
    "            self.pipe.tokenizer.eos_token_id,\n",
    "            self.pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            ]\n",
    "        \n",
    "        \n",
    "        self.default_config = {\n",
    "            'do_sample': True,\n",
    "            'max_new_tokens': self.MAX_RESPONSE_LENGTH,\n",
    "            'temperature': 0.6,\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "        \"\"\"\n",
    "        print(\"Processing request\")\n",
    "        \n",
    "        if data[\"system_prompt\"]:\n",
    "            system_prompt = data[\"system_prompt\"]\n",
    "        else:\n",
    "            system_prompt = \"You are a pirate chatbot who always responds in pirate speak!\"\n",
    "            \n",
    "        config = self.default_config.copy()\n",
    "        \n",
    "        # Update config dic if the user added a config dict\n",
    "        if data[\"config\"]:\n",
    "            config.update(data[\"config\"])\n",
    "            \n",
    "        # Create the full prompt\n",
    "        formatted_messages = []\n",
    "        for message in self.messages:\n",
    "            # Use format() method to format the content of each dictionary\n",
    "            formatted_content = message[\"content\"].format(\n",
    "                system_prompt=system_prompt, user_prompt=data[\"prompt\"]\n",
    "            )\n",
    "            # Append the formatted content to the new list\n",
    "            formatted_messages.append({\"role\": message[\"role\"], \"content\": formatted_content})\n",
    "\n",
    "        full_prompt = self.pipe.tokenizer.apply_chat_template(\n",
    "            formatted_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        print(f\"The input for the model is this full prompt \\n:{full_prompt}\")\n",
    "        \n",
    "        # Generate text\n",
    "        sequences = self.pipe(\n",
    "            full_prompt,\n",
    "            eos_token_id=self.terminators,\n",
    "            **config\n",
    "        )\n",
    "\n",
    "        response = sequences[0][\"generated_text\"]\n",
    "\n",
    "        # Here we set our output parameters in the form of a json\n",
    "        return {\"output\": response, \n",
    "                \"input\": full_prompt, \n",
    "                \"used_config\": config\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a UbiOps deployment\n",
    "\n",
    "Now we can create the deployment, where we define the in- and outputs of the model. Each deployment can have multiple versions.\n",
    "For each, version you can use a different deployed code, environment, instance type, among other settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the deployment\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    input_type=\"structured\",\n",
    "    output_type=\"structured\",\n",
    "    input_fields=[\n",
    "        {\"name\": \"prompt\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"system_prompt\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"config\", \"data_type\": \"dict\"},\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {\"name\": \"output\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"input\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"used_config\", \"data_type\": \"dict\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "api.deployments_create(project_name=PROJECT_NAME, data=deployment_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a deployment version\n",
    "\n",
    "Now we will create a version of the deployment. For the version, we need to define the name, environment, instance type \n",
    "(CPU or GPU) as well as the size of the instance.\n",
    "\n",
    "For this model it is recommended to use a GPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment=ENVIRONMENT_NAME,\n",
    "    instance_type=\"16384mb_l4\",\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    maximum_idle_time=600,  # = 10 minutes\n",
    "    request_retention_mode=\"full\",\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=DEPLOYMENT_NAME, data=version_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package and upload the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "deployment_code_archive = shutil.make_archive(\n",
    "    deployment_code_dir, \"zip\", deployment_code_dir\n",
    ")\n",
    "\n",
    "upload_response = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file=deployment_code_archive,\n",
    ")\n",
    "print(upload_response)\n",
    "\n",
    "# Check if the deployment is finished building. This can take a few minutes\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=upload_response.revision,\n",
    "    stream_logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can send requests to our deployment version, the environment has to be finished building. Note that building the\n",
    " environment might take a while  as UbiOps needs to download and install all the packages and dependencies. The environment\n",
    " only needs to be built once, the next time that an instance type is spun up for our deployment the dependencies do not have\n",
    " to be installed anymore. You can toggle off `stream_logs` to not stream logs of the build process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment variable\n",
    "\n",
    "Here we create environment variables for the Huggingface token. We need this token to allow us to download the Llama model\n",
    "from Huggingface, since it's behind a gated repo. \n",
    "\n",
    "If you want to use a different version of Llama 3, you can also add an environment variable for the `model_id` by adding \n",
    "this code to the code cell below:\n",
    "\n",
    "<details>\n",
    " <summary>Click here to see the code that creates an environment variable for the `model_id`</summary>\n",
    "\n",
    "```python\n",
    "MODEL_ID = \"ENTER THE MODEL_ID HERE\"  # You can change this parameter if you want to use a different model from Huggingface.\n",
    "\n",
    "\n",
    "api_response = api.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    data=ubiops.EnvironmentVariableCreate(\n",
    "        name=\"model_id\", value=MODEL_ID, secret=False\n",
    "    ),\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = api.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    data=ubiops.EnvironmentVariableCreate(name=\"model_id\", value=HF_TOKEN, secret=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calling the Llama 3 8B deployment API endpoint\n",
    "\n",
    "Our deployment is now ready to process requests! We can send requests to the deployment using either the \n",
    "[`deployment-requests-create` or `batch-deployment-requests-create`](https://ubiops.com/docs/requests/#request-types) \n",
    "API endpoint. During this step a node will be spun up, and the model will be downloaded from Huggingface. Hence why this \n",
    "step can take a while. You can monitor the progress of the process in the [logs](https://ubiops.com/docs/monitoring/logging/). \n",
    "Subsequent requests to the deployment will be handled faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a request using the default `system_prompt` and `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"prompt\": \"tell me a joke\", \"system_prompt\": \"\", \"config\": {}}\n",
    "\n",
    "api.deployment_requests_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=DEPLOYMENT_NAME, data=data, timeout=3600\n",
    ").result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a request using other values for the `system_prompt` and `config`.\n",
    "\n",
    "For this request, we will instruct the LLM to translate English texts into the style of Shakespearean. We will let the model\n",
    "be more creative with generating sequences by lowering the `temperature` parameter. The text used for this example is shown\n",
    "in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In the village of Willowbrook lived a girl named Amelia, known for her kindness and curiosity. One autumn day, she ventured into the forest and stumbled upon an old cottage filled with dusty tomes of magic. Amelia delved into the ancient spells, discovering her own hidden powers. As winter approached, a darkness loomed over the village. Determined to protect her home, Amelia confronted the source of the darkness deep in the forest. With courage and magic, she banished the shadows and restored peace to Willowbrook., Emerging triumphant, Amelia returned home, her spirit ablaze with newfound strength. From that day on, she was known as the brave sorceress who saved Willowbrook, a legend of magic and courage that echoed through the ages.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"prompt\": text,\n",
    "    \"system_prompt\": \"You are a friendly chatbot that translates texts into the style of Shakespearean.\",\n",
    "    \"config\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.5,\n",
    "    },\n",
    "}\n",
    "\n",
    "api.deployment_requests_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=DEPLOYMENT_NAME, data=data, timeout=3600\n",
    ").result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it! You now have your own on-demand, scalable Llama-3-8B-Instruct-v0.2 model running in the cloud, with a REST API that you can reach from anywhere!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
