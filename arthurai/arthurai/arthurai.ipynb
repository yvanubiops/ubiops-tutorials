{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arthur, UbiOps and XGBoost\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note: This notebook runs on Python 3.8 and uses UbiOps CLient Library 3.15.0.\n",
    "\n",
    "In this notebook we will show you the following:\n",
    "\n",
    "- How to create a deployment that uses a built xgboost model to make predictions on the prices of houses based on some criteria about the house and deploy that on [UbiOps](https://ubiops.com/)\n",
    "- How to integrate with the [Arthur](https://arthur.ai/) platform to monitor your machine learning deployments.\n",
    "\n",
    "This example uses the House Sales in King County, USA Dataset. [Link to the dataset](https://kaggle.com/datasets/harlfoxem/housesalesprediction)\n",
    "\n",
    "\n",
    "If you run this entire notebook after filling in your access tokens, the xgboost deployment will be deployed to your UbiOps and Arthur environments. You can thus check your environment after running to explore. You can also check the individual steps in this notebook to see what we did exactly and how you can adapt it to your own use case.\n",
    "\n",
    "We recommend to run the cells step by step, as some cells can take a few minutes to finish. You can run everything in one go as well and it will work, just allow a few minutes for building the individual deployments.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a connection with your UbiOps environment\n",
    "\n",
    "Add your API token and your project name. We provide a deployment name and deployment version name. Afterwards we initialize the client library. This way we can deploy the XGBoost model to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:40:55.221596Z",
     "start_time": "2021-05-12T15:40:55.218324Z"
    }
   },
   "outputs": [],
   "source": [
    "API_TOKEN = \"<YOUR UBIOPS API TOKEN>\" # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"<YOUR PROJECT>\"\n",
    "\n",
    "DEPLOYMENT_NAME = 'xgboost-arthur-deployment'\n",
    "DEPLOYMENT_VERSION = 'v1'\n",
    "\n",
    "# Import all necessary libraries\n",
    "import shutil\n",
    "import os\n",
    "import ubiops\n",
    "\n",
    "client = ubiops.ApiClient(ubiops.Configuration(api_key={'Authorization': API_TOKEN}, \n",
    "                                               host='https://api.ubiops.com/v2.1'))\n",
    "api = ubiops.CoreApi(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model\n",
    "\n",
    "This example will be based on [this kaggle](https://kaggle.com/code/mburakergenc/predictions-with-xgboost-and-linear-regression) about making predictions with XGboost and Linear Regression.\n",
    "\n",
    "Since this document will be focused on the deploying side of the ML process. We will not cover the development of the model in-depth and make use of the pre-trained model below.\n",
    "\n",
    "After running this cell you should see a comparision between the `sklearn` model and the `xgboost` model regarding the accuracy score and the RMSE (Root Mean Square Error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first install the python packages we will need for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "!pip install xgboost\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T16:02:55.274865Z",
     "start_time": "2021-05-12T16:02:54.507504Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import tree, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import joblib\n",
    "\n",
    "# Read the data into a data frame\n",
    "data = pd.read_csv('kc_house_data.csv').astype(dtype={'id': str})\n",
    "\n",
    "# Train a simple linear regression model\n",
    "regr = linear_model.LinearRegression()\n",
    "input_columns = ['sqft_living','grade', 'sqft_above', 'sqft_living15','bathrooms','view','sqft_basement','lat','waterfront','yr_built','bedrooms']\n",
    "\n",
    "# Create train test sets\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "X_train, y_train = train_data[input_columns].to_numpy(), train_data['price'].to_numpy()\n",
    "X_test, y_test = test_data[input_columns].to_numpy(), test_data['price'].to_numpy()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2)\n",
    "# Train the model\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Check how the sklearn model scores on accuracy on our test set\n",
    "sklearn_score = regr.score(X_test,y_test)\n",
    "# Print the score of the sklearn model (Not great)\n",
    "print(f'Score of the sklearn model: {sklearn_score}')\n",
    "\n",
    "# Calculate the Root Mean Squared Error\n",
    "print(\"RMSE of the sklearn model: %.2f\"\n",
    "      % math.sqrt(np.mean((regr.predict(X_test) - y_test) ** 2)))\n",
    "\n",
    "# Let's try XGboost algorithm to see if we can get better results\n",
    "xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "\n",
    "# Train the model\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions using the xgboost model\n",
    "predictions = xgb.predict(X_test)\n",
    "\n",
    "\n",
    "# Check how the xgboost model scores on accuracy on our test set\n",
    "xgboost_score = explained_variance_score(predictions,y_test)\n",
    "\n",
    "print(f'Score of the xgboost model {xgboost_score}')\n",
    "\n",
    "# Calculate the Root Mean Squared Error\n",
    "print(\"RMSE of the xgboost model: %.2f\"\n",
    "      % math.sqrt(np.mean((predictions - y_test) ** 2)))\n",
    "\n",
    "\n",
    "#save model\n",
    "joblib.dump(xgb, 'xgboost-deployment/xgboost_model.joblib') \n",
    "print('XGBoost model built and saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering the Model with Arthur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a connection to Arthur and then define the model, using the training data to infer the model input schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:41:05.855437Z",
     "start_time": "2021-05-12T15:41:05.853250Z"
    }
   },
   "outputs": [],
   "source": [
    "from arthurai import ArthurAI\n",
    "from arthurai.common.constants import Stage, InputType, OutputType, ValueType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:41:07.411023Z",
     "start_time": "2021-05-12T15:41:06.394358Z"
    }
   },
   "outputs": [],
   "source": [
    "ARTHUR_URL = \"https://app.arthur.ai\"\n",
    "ARTHUR_ACCESS_KEY = \"<YOUR ARTHUR API KEY>\" # Fill this in\n",
    "\n",
    "connection = ArthurAI(url=ARTHUR_URL, access_key=ARTHUR_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:41:42.151018Z",
     "start_time": "2021-05-12T15:41:42.070229Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the model schema\n",
    "arthur_model = connection.model(partner_model_id=\"UbiOps House Prices\",\n",
    "                                input_type=InputType.Tabular,\n",
    "                                output_type=OutputType.Regression,\n",
    "                                is_batch=True)\n",
    "\n",
    "arthur_model.from_dataframe(train_data[input_columns], Stage.ModelPipelineInput)\n",
    "arthur_model.add_regression_output_attributes({\"price\": \"price_gt\"}, value_type=ValueType.Float)\n",
    "arthur_model.review()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe above represents how the model will look to Arthur, and the format of the data it will expect. Notice how it detected some columns as categorical (such as Waterfront and View).\n",
    "\n",
    "Now we can save the model to Arthur, and store the Arthur Model ID to be used by our deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:41:47.697985Z",
     "start_time": "2021-05-12T15:41:44.681927Z"
    }
   },
   "outputs": [],
   "source": [
    "arthur_model_id = arthur_model.save()\n",
    "with open(\"xgboost-deployment/arthur-model-id.txt\", 'w') as f:\n",
    "    f.write(arthur_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:55:58.313925Z",
     "start_time": "2021-05-07T14:55:58.310433Z"
    }
   },
   "source": [
    "Finally, we'll upload the data we used to train the model as a reference set. Future data sent to the model will be compared to this reference set, to measure how much it has drifted from the types of inputs the model was built from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:41:55.863396Z",
     "start_time": "2021-05-12T15:41:55.823851Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_df = train_data[['price'] + input_columns].rename(columns={'price': 'price_gt'})\n",
    "ref_df['price'] = xgb.predict(ref_df[input_columns].to_numpy())\n",
    "ref_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:41:59.957091Z",
     "start_time": "2021-05-12T15:41:59.316321Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arthur_model.set_reference_data(data=ref_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the XGboost deployment\n",
    "Now that we have our model saved it is time to create a deployment in UbiOps that will make use of it.\n",
    "\n",
    "In the cell below you can view the deployment.py which will take data about the house we wish to predict the price of. As you can see in the initialization step we load the model we created earlier, then in the request method we make use of it to make a prediction. Input to this model is:\n",
    "\n",
    "* data: a csv file with the house data to predict its price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T17:15:14.498469Z",
     "start_time": "2021-05-11T17:15:14.494966Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile xgboost-deployment/deployment.py\n",
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "from uuid import uuid4\n",
    "from joblib import load\n",
    "from arthurai.core.decorators import log_prediction\n",
    "from arthurai import ArthurAI\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "\n",
    "        :param str base_directory: absolute path to the directory where the deployment.py file is located\n",
    "        :param dict context: a dictionary containing details of the deployment that might be useful in your code.\n",
    "            It contains the following keys:\n",
    "                - deployment (str): name of the deployment\n",
    "                - version (str): name of the version\n",
    "                - input_type (str): deployment input type, either 'structured' or 'plain'\n",
    "                - output_type (str): deployment output type, either 'structured' or 'plain'\n",
    "                - environment (str): the environment in which the deployment is running\n",
    "                - environment_variables (str): the custom environment variables configured for the deployment.\n",
    "                    You can also access those as normal environment variables via os.environ\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising xgboost model\")\n",
    "\n",
    "        XGBOOST_MODEL = os.path.join(base_directory, \"xgboost_model.joblib\")\n",
    "        self.model = load(XGBOOST_MODEL)\n",
    "\n",
    "        with open(\"arthur-model-id.txt\", 'r') as f:\n",
    "            print(\"Initializing Arthur connection\")\n",
    "            self.arthur_model = ArthurAI().get_model(f.read())\n",
    "            print(\"Successfully retrieved Arthur model\")\n",
    "\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "\n",
    "        :param dict/str data: request input data. In case of deployments with structured data, a Python dictionary\n",
    "            with as keys the input fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain input, it is a string.\n",
    "        :return dict/str: request output. In case of deployments with structured output data, a Python dictionary\n",
    "            with as keys the output fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain output, it is a string. In this example, a dictionary with the key: output.\n",
    "        \"\"\"\n",
    "        print('Loading data')\n",
    "        batch = pd.read_csv(data['data']).astype({'id': str})\n",
    "        batch_id = str(uuid4()).split('-')[-1]\n",
    "\n",
    "        print(\"Predictions being made\")\n",
    "        batch['price'] = self.model.predict(batch.drop(columns=['id']).to_numpy())\n",
    "\n",
    "        print(\"Sending batch to Arthur\")\n",
    "        inference_data = [{'inference_timestamp': datetime.datetime.now(pytz.utc),\n",
    "                           'partner_inference_id': row['id'],\n",
    "                           'batch_id': batch_id,\n",
    "                           'inference_data': {k: row[k] for k in row.keys() if k != 'id'}}\n",
    "                           for row in batch.to_dict(orient='records')]\n",
    "        self.arthur_model.send_inferences(inference_data)\n",
    "        \n",
    "        # Writing the prediction to a csv for further use\n",
    "        print('Writing prediction to csv')\n",
    "        batch['price'].to_csv('prediction.csv', header = ['house_prices'], index_label= 'index')\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": 'prediction.csv'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deploying to UbiOpsÂ¶\n",
    "\n",
    "Now we have all the pieces we need to create our deployment on UbiOps. In the cell below a deployment is being created, then a version of the deployment is created and the deployment code is zipped and uploaded to that version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:42:15.696277Z",
     "start_time": "2021-05-12T15:42:13.944845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the deployment\n",
    "\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    description='XGBoost deployment',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        {'name':'data', 'data_type':'file'},\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {'name':'prediction', 'data_type':'file'},\n",
    "    ],\n",
    "    labels={'demo': 'xgboost'}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Add Arthur environment variables\n",
    "api.deployment_environment_variables_create(project_name=PROJECT_NAME,\n",
    "                                            deployment_name=DEPLOYMENT_NAME,\n",
    "                                            data=ubiops.EnvironmentVariableCreate(name='ARTHUR_ENDPOINT_URL',\n",
    "                                                                                  value=ARTHUR_URL,\n",
    "                                                                                  secret=False))\n",
    "api.deployment_environment_variables_create(project_name=PROJECT_NAME,\n",
    "                                            deployment_name=DEPLOYMENT_NAME,\n",
    "                                            data=ubiops.EnvironmentVariableCreate(name='ARTHUR_API_KEY',\n",
    "                                                                                  value=ARTHUR_ACCESS_KEY,\n",
    "                                                                                  secret=True))\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment='python3-8',\n",
    "    instance_type='512mb',\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800, # = 30 minutes\n",
    "    request_retention_mode='none' # we don't need request storage in this example\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Zip the deployment package\n",
    "shutil.make_archive('xgboost-deployment', 'zip', '.', 'xgboost-deployment')\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='xgboost-deployment.zip'\n",
    ")\n",
    "\n",
    "# Check if the deployment is finished building. This can take a few minutes\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=file_upload_result.revision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a request and exploring further\n",
    "\n",
    "Finally we'll generate some sample data from our test set to use in the Web UI. After running the cell below take a look at the generated CSV files in the `sample_data` folder: we'll generate three batches of sample data and three files containing the true prices, all identified by the dataset's unique row IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T16:05:27.974923Z",
     "start_time": "2021-05-12T16:05:27.956085Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "makedirs('./sample_data', exist_ok=True)\n",
    "\n",
    "NUM_BATCHES = 3\n",
    "for batch_num in range(1, NUM_BATCHES+1):\n",
    "    # choose a random set of indices for this batch\n",
    "    batch_size = int(np.random.normal(100, 30))\n",
    "    indices = np.random.choice(np.arange(len(test_data)), batch_size)\n",
    "    \n",
    "    # write out the input data with the ID to a CSV\n",
    "    test_data.iloc[indices][input_columns + ['id']].to_csv(f'./sample_data/sample_batch_{batch_num}.csv', index=False)\n",
    "    \n",
    "    # write out the ground truth with the ID to a CSV, renaming the column 'price' to the ground truth 'price_gt'\n",
    "    (test_data.iloc[indices][['id', 'price']].rename(columns={'price': 'price_gt'})\n",
    "         .to_csv(f'./sample_data/ground_truth_batch_{batch_num}.csv', index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:15:21.076905Z",
     "start_time": "2021-05-12T15:15:21.068953Z"
    }
   },
   "source": [
    "You can go ahead to the UbiOps Web App and take a look in the user interface at what you have just built. Check out the `sample_data` directory and try uploading the `sample_batch_1.csv` file. You can then download the generated `predictions.csv` but they'll also be logged with Arthur.\n",
    "\n",
    "## Sending actuals\n",
    "\n",
    "Finally, we'll tell Arthur what the true price values were, so that we can compute accuracy metrics. We can send this ground truth at the same time as predictions, but we'll demonstrate sending it after the fact to simulate the real-world experience of receiving the true label sometime in the future.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T16:13:19.731285Z",
     "start_time": "2021-05-12T16:13:19.727399Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime, pytz\n",
    "\n",
    "def send_batch_ground_truth(filename):\n",
    "    df = pd.read_csv(filename).astype({'id': str})\n",
    "    ground_truth_data = []\n",
    "    for row in df.itertuples():\n",
    "        ground_truth_data.append({'partner_inference_id': row.id,\n",
    "                                  'ground_truth_timestamp': datetime.datetime.now(pytz.utc),\n",
    "                                  'ground_truth_data': {\n",
    "                                      'price_gt': row.price_gt\n",
    "                                  }})\n",
    "    arthur_model.update_inference_ground_truths(ground_truth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T16:13:20.559703Z",
     "start_time": "2021-05-12T16:13:20.339400Z"
    }
   },
   "outputs": [],
   "source": [
    "send_batch_ground_truth('./sample_data/ground_truth_batch_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T19:32:56.677823Z",
     "start_time": "2021-05-12T19:32:56.270338Z"
    }
   },
   "outputs": [],
   "source": [
    "# send_batch_ground_truth('./sample_data/ground_truth_batch_2.csv')\n",
    "# send_batch_ground_truth('./sample_data/ground_truth_batch_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done! Let's close the client properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "That's it! We've walked through building a model, creating it as a deployment with UbiOps, registering the model with Arthur, and sending data. Head over to the [Arthur UI](https://app.arthur.ai) to see the data, predictions, and analysis.\n",
    "\n",
    "You can use this notebook to base your own deployments on. Just adapt the code in the deployment packages and alter the input and output fields as you wish and you should be good to go. \n",
    "\n",
    "For any questions, feel free to reach out to UbiOps via the [customer service portal](https://ubiops.atlassian.net/servicedesk/customer/portals) or Arthur via the chat on [the homepage](https://arthur.ai)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
