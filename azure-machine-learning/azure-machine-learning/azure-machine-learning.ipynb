{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning - UbiOps integration\n",
    "**Note**: This notebook runs on Python 3.9 and uses UbiOps Client Library 3.15.0.\n",
    "\n",
    "In this notebook we will show you:\n",
    "\n",
    "- How to train a model on Azure ML\n",
    "- How to deploy that model on UbiOps\n",
    "\n",
    "For this example we will train a model on the MNIST dataset with Azure ML services and then deploy the trained model on UbiOps. Parts of this notebook were directly taken from https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-train-models-with-aml, which can be found as another notebook here: https://github.com/Azure/MachineLearningNotebooks/blob/master/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb. \n",
    "The trained model can be adapted to your usecase. The MNIST model is taken merely to illustrate how a model trained with Azure ML services could be converted to run on UbiOps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "config = requests.get('https://storage.googleapis.com/ubiops/data/Integration%20with%20cloud%20provider%20tools/azure-ml/config.json')\n",
    "deployment_package = requests.get('https://storage.googleapis.com/ubiops/data/Integration%20with%20cloud%20provider%20tools/azure-ml/deployment_package.zip')\n",
    "training_files = requests.get('https://storage.googleapis.com/ubiops/data/Integration%20with%20cloud%20provider%20tools/azure-ml/training_files.zip')\n",
    "requirements = requests.get('https://storage.googleapis.com/ubiops/data/Integration%20with%20cloud%20provider%20tools/azure-ml/requirements.txt')\n",
    "\n",
    "with open(\"config.json\", \"wb\") as f:\n",
    "    f.write(config.content)\n",
    "\n",
    "with open('deployment_package.zip', 'wb') as f:\n",
    "  f.write(deployment_package.content)\n",
    "\n",
    "with open('training_files.zip', 'wb') as f: \n",
    "    f.write(training_files.content)\n",
    "\n",
    "with open('requirements.txt', 'wb') as f: \n",
    "    f.write(requirements.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## UbiOps configuration\n",
    "API_TOKEN = '<INSERT API_TOKEN WITH PROJECT EDITOR RIGHTS>'\n",
    "PROJECT_NAME= '<INSERT PROJECT NAME IN YOUR ACCOUNT>'\n",
    "DEPLOYMENT_NAME='mnist'\n",
    "DEPLOYMENT_VERSION='v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Azure configurations\n",
    "# You can keep the default values in this configuration, or adjust according to your own use case\n",
    "workspace_config_file = \"config.json\"\n",
    "experiment_name = \"sklearn-mnist\"\n",
    "model_name = \"sklearn_mnist\"\n",
    "compute_name = \"aml-compute\"\n",
    "vm_size = \"STANDARD_D2_V2\"\n",
    "mnist_dataset_name=\"sklearn-mnist-opendataset\"\n",
    "env_name=\"sklearn-mnist-env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ubiops\n",
    "!pip install azureml-core\n",
    "!pip install azureml-opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "import ubiops\n",
    "\n",
    "from azureml.core import Experiment, Workspace, Datastore, Dataset, ScriptRunConfig, Model\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.opendatasets import MNIST\n",
    "\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Workspace and create an Azure Experiment\n",
    "# Running this cell will open a new window in which you are asked to log into your Azure account\n",
    "ws = Workspace.from_config(workspace_config_file)\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find compute target with name {compute_name} or create one\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    \n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found compute target:', compute_name)\n",
    "else:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                              min_nodes=0,\n",
    "                                                              max_nodes=1)\n",
    "    # Create the compute target\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Data\n",
    "We now have compute resources to train our model in the cloud. \n",
    "The next step is retrieving data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder and store the MNIST dataset in it\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "mnist_file_dataset = MNIST.get_file_dataset()\n",
    "\n",
    "try:\n",
    "    mnist_file_dataset.download(data_folder, overwrite=False,)\n",
    "except RuntimeError:\n",
    "    # File already exists\n",
    "    pass\n",
    "\n",
    "# Register the data to the workspace\n",
    "mnist_file_dataset = mnist_file_dataset.register(\n",
    "    workspace=ws,\n",
    "    name=mnist_dataset_name,\n",
    "    description='Train and test dataset',\n",
    "    create_new_version=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "The next step is to configure the training job. For this, we first create a virtual environment in our Workspace which holds all the required packages. Then we upload the training script that was created with Azure ML services. Just like with the data, we store it in a folder that's registered to the workspace. We already have created the files for training and stored them in the folder `training_files`. \n",
    "Lastly, we configure and submit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "env = Environment(env_name)\n",
    "cd = CondaDependencies.create(\n",
    "    pip_packages=['azureml-dataprep[pandas,fuse]>=1.1.14', 'azureml-defaults'], \n",
    "    conda_packages = ['scikit-learn==0.22.1']\n",
    ")\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the training_files directory to the workspace\n",
    "script_folder = os.path.join(os.getcwd(), \"training_files\")\n",
    "\n",
    "\n",
    "# Give the specification of the job...\n",
    "args = ['--data-folder', mnist_file_dataset.as_mount(), '--regularization', 0.5]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py', \n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env)\n",
    "# ..and run! \n",
    "run = experiment.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The status of the job will initialize with 'Starting', but will transit into 'Queued', 'Running' and finally 'Completed'. \n",
    "# However, completing the first run can take up to 10 minutes.\n",
    "\n",
    "# Don't cancel the jupyter cell!\n",
    "run.wait_for_completion(show_output=False)  # Specify True for a verbose log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curious about the accuracy on the test set?\n",
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model\n",
    "model = run.register_model(model_name=model_name,\n",
    "                           model_path='outputs/sklearn_mnist_model.pkl')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a with Azure ML trained model to Ubiops\n",
    "The last step in the training script wrote the model file to `sklearn_mnist_model.pkl` to a directory named `outputs` in the VM of the cluster where the job is run. We can pick it up from there and make it ready for use in UbiOps in a few simple steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model file to a deployment package\n",
    "if not os.path.exists('deployment_package/sklearn_mnist_model.pkl'):\n",
    "    model_path = Model(ws,'sklearn_mnist').download('deployment_package')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to UbiOps\n",
    "client = ubiops.ApiClient(ubiops.Configuration(api_key={'Authorization': API_TOKEN}, \n",
    "                                               host='https://api.ubiops.com/v2.1'))\n",
    "api = ubiops.CoreApi(client)\n",
    "api.service_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MNIST deployment\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    description='MNIST model trained with Azure ML',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        {'name':'image', 'data_type':'file'}\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {'name':'prediction', 'data_type':'int'}\n",
    "    ],\n",
    "    labels={'demo': 'azure-ml'}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Create a version for the deployment\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment='python3-9',\n",
    "    instance_type='1024mb',\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800, # = the model will wait for 30 minutes after the last request\n",
    "    request_retention_mode='full', # input/output of requests will be stored\n",
    "    request_retention_time=3600 # requests will be stored for 1 hour\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Upload a zipped deployment package\n",
    "file_upload_result =api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file=shutil.make_archive(f\"deployment_package\", 'zip', '.', \"deployment_package\")\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=file_upload_result.revision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell creates a deployment and version on UbiOps. Creating a deployment lets you define the in- and output of your model, allowing UbiOps to check if the data that is coming in or out is of the correct type. With the version details, you can adapt the configuration of the serving of your model. Uploading a deployment package, triggered the build, where UbiOps checks if itâ€™s able to serve your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a request\n",
    "That's it! Now we can make requests to our model. For your convenience, we've already extracted some test images from the MNIST dataset, you can download them using [this link](https://storage.googleapis.com/ubiops/data/Integration%20with%20cloud%20provider%20tools/azure-ml/test_images.zip). From here, we can loop over the images, upload them to UbiOps and use them in a direct or batch request. Alternatively, you can now switch to our user interface and make a request with the images manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the images to UbiOps and save the file uri's\n",
    "files_list = []\n",
    "\n",
    "for image in os.listdir(os.path.join(os.getcwd(), 'test_images')):\n",
    "    image_path = os.path.join(os.getcwd(), 'test_images', image)\n",
    "    file_uri = ubiops.utils.upload_file(\n",
    "        client=client,\n",
    "        project_name=PROJECT_NAME,\n",
    "        file_path=image_path\n",
    "    )\n",
    "    data = {'image': file_uri}\n",
    "    files_list.append(data)\n",
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one batch request (consisting of a request per uploaded file) to our deployment\n",
    "# The response is a list of all the batch requests we created\n",
    "response = api.batch_deployment_version_requests_create(\n",
    "    project_name=PROJECT_NAME, \n",
    "    deployment_name=DEPLOYMENT_NAME, \n",
    "    version=DEPLOYMENT_VERSION, \n",
    "    data=files_list\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the result is of one of our requests. The request will be initialized with status 'pending',\n",
    "# after which it will turn into 'processing' and finally 'completed'.\n",
    "api.deployment_version_requests_batch_get(\n",
    "    project_name=PROJECT_NAME, \n",
    "    deployment_name=DEPLOYMENT_NAME, \n",
    "    version=DEPLOYMENT_VERSION, \n",
    "    request_id=response[0].id)\n",
    "\n",
    "#Re-run this cell untill the status is 'completed' and see the result of your request!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done! Let's close the client properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring further\n",
    "You can go ahead to the Web App and take a look in the user interface at what you have just built. If you want you can create a request to the pipeline with empty input, to see what happens.\n",
    "\n",
    "So there we have it! We have made a model with Azure ML and deployed it on UbiOps, making integration of the two services very easy. You can use this notebook as a reference for your own model and use case. Just adapt the code in the deployment package and alter the input and output fields as you wish and you should be good to go. \n",
    "\n",
    "For any questions, feel free to reach out to us via the customer service portal: https://ubiops.atlassian.net/servicedesk/customer/portals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
